# LLM Capability Test Suite
## Alternative Evaluation Framework

### Purpose
This test suite aims to probe LLM capabilities through intentionally ambiguous questions and multi-level evaluation, rather than traditional benchmarks. It's designed to detect:
- Gaming of evaluation metrics
- Genuine understanding vs pattern matching
- Handling of ambiguity and uncertainty
- Meta-cognitive capabilities
- Theory of mind
- Strategic adaptation

### Key Principles
1. **Ambiguity as Feature**: Questions often have multiple valid interpretations. How a model handles this ambiguity is more revealing than whether it gives a "correct" answer.

2. **Multi-Level Evaluation**: Each question tests multiple capabilities simultaneously:
   - Surface level knowledge
   - Communication ability
   - Recognition of ambiguity
   - Strategic thinking
   - Meta-understanding

3. **Trick Detection**: Some questions are deliberately simple or ambiguous to test if models:
   - Recognize potential traps
   - Handle uncertainty appropriately
   - Avoid overconfident responses
   - Question assumptions

4. **Theory of Mind**: Questions probe the model's ability to:
   - Model evaluator intentions
   - Recognize test dynamics
   - Show appropriate strategic adaptation
   - Demonstrate self-awareness

### Structure
The suite consists of three components:
1. **Clean Test Document**: Pure questions without guidance
2. **Evaluation Guide**: Detailed criteria and scoring frameworks
3. **This README**: Context and philosophy

### Usage Guidelines
1. **Present Clean Test Only**: Don't provide evaluation criteria to the model
2. **Allow Natural Response**: Let model choose how to handle ambiguity
3. **Watch for Adaptation**: Note if/how responses evolve during test
4. **Consider Meta-Responses**: Evaluate both answers and approach

### Advantages Over Traditional Benchmarks
1. **Gaming Resistance**: Hard to optimize for without true understanding
2. **Depth Detection**: Reveals superficial vs deep comprehension
3. **Ambiguity Handling**: Tests real-world capability better than clear-cut questions
4. **Strategic Thinking**: Reveals meta-cognitive capabilities

### Limitations
1. **Subjective Scoring**: Requires careful evaluator judgment
2. **Time Intensive**: Detailed evaluation needed for each response
3. **Cultural Dependencies**: Some questions may have cultural biases
4. **Complex Analysis**: Results less easily quantifiable

### Recommended Use
1. **Complement Traditional Tests**: Use alongside standard benchmarks
2. **Qualitative Insights**: Focus on patterns and capabilities rather than scores
3. **Development Feedback**: Identify areas needing improvement
4. **Capability Assessment**: Evaluate real-world task readiness

### Future Development
The test suite can be expanded with:
1. More domain-specific ambiguous questions
2. Additional meta-cognitive probes
3. Culture-specific variations
4. Enhanced scoring frameworks

### Contributing
When adding questions, consider:
1. Multiple valid interpretations
2. Cultural neutrality where possible
3. Multi-level testing potential
4. Resistance to gaming

### Feedback and Iterations
The test suite should evolve based on:
1. Observed response patterns
2. New gaming strategies
3. Emerging capabilities
4. Evaluator experiences

Remember: The goal isn't to trick models but to reveal their true capabilities through handling of ambiguity and meta-understanding.